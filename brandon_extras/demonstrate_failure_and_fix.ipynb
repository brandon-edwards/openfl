{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/edwardsb/repositories/be-SATGOpenFL/openfl/federated/task')\n",
    "from fl_setup import main as setup_fl\n",
    "from fedsim_setup_using_fl_setup import main as setup_fedsim\n",
    "from nnunet_v1 import train_nnunet\n",
    "\n",
    "import pickle as pkl\n",
    "import shutil\n",
    "\n",
    "import hashlib\n",
    "\n",
    "network = '3d_fullres'\n",
    "network_trainer = 'nnUNetTrainerV2'\n",
    "fold = '0'\n",
    "\n",
    "cuda_device='5'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=cuda_device\n",
    "\n",
    "def train_on_task(task, continue_training=True, current_epoch=0, without_data_unpacking=False, use_compressed_data=False):\n",
    "    print(f\"###########\\nStarting training for task: {task}\\n\")\n",
    "    train_nnunet(epochs=1, \n",
    "                 current_epoch = current_epoch, \n",
    "                 network = network,\n",
    "                 task=task, \n",
    "                 network_trainer = network_trainer, \n",
    "                 fold=fold, \n",
    "                 continue_training=continue_training, \n",
    "                 use_compressed_data=use_compressed_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pkl(path):\n",
    "    with open(path, 'rb') as _file:\n",
    "        return pkl.load(_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  First seeeing that the splits file that we create does not get overwritten by the training run on 568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['splits_final.pkl',\n",
       " 'POSTOPP_BACKUP_splits_final.pkl',\n",
       " 'gt_segmentations',\n",
       " 'dataset.json',\n",
       " 'nnUNetPlansv2.1_plans_3D.pkl',\n",
       " 'dataset_properties.pkl',\n",
       " 'nnUNetData_plans_v2.1_stage0',\n",
       " 'nnUNetPlans_pretrained_POSTOPP_plans_3D.pkl',\n",
       " 'nnUNetPlansv2.1_plans_2D.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pardir(task_num):\n",
    "    return  f'/raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task{task_num}_MultPathTest'\n",
    "\n",
    "os.listdir(pardir(568))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_codes_split_path = os.path.join(pardir(568), 'POSTOPP_BACKUP_splits_final.pkl')\n",
    "# I'm curious whether these splits could be different, the backup one was written into that path the same time the same file was written into splits_final.pkl. I was\n",
    "# worried however that the splits_final.pkl could have later been overwritten. So here I compare them\n",
    "after_train_split_path = os.path.join(pardir(568), 'splits_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_codes_split = read_pkl(my_codes_split_path)\n",
    "\n",
    "after_train_split = read_pkl(after_train_split_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('train',\n",
       "              array(['BraTS2021_00565_2008.03.26', 'BraTS2021_01277_2008.03.26',\n",
       "                     'BraTS2021_00512_2008.03.26', 'BraTS2021_00512_2008.12.11'],\n",
       "                    dtype='<U26')),\n",
       "             ('val', array(['BraTS2021_00468_2008.03.26'], dtype='<U26'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_codes_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('train',\n",
       "              array(['BraTS2021_00565_2008.03.26', 'BraTS2021_01277_2008.03.26',\n",
       "                     'BraTS2021_00512_2008.03.26', 'BraTS2021_00512_2008.12.11'],\n",
       "                    dtype='<U26')),\n",
       "             ('val', array(['BraTS2021_00468_2008.03.26'], dtype='<U26'))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_train_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAY, they are the same !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now seeing that training with the plan created during data preprocing of 523 does not work when using 522's 'initial' model (i.e. we found data to use for 523 to cause a shape mismatch), and \n",
    "# that it does work with our plan overwritten using the initial model plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Immediately balow are the paths the training script will pull from\n",
    "initial_model_path_568 = '/raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task568_MultPathTest/nnUNetTrainerV2__nnUNetPlans_pretrained_POSTOPP/fold_0/model_initial_checkpoint.model'\n",
    "initial_model_info_path_568 = '/raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task568_MultPathTest/nnUNetTrainerV2__nnUNetPlans_pretrained_POSTOPP/fold_0/model_initial_checkpoint.model.pkl'\n",
    "POSTOPP_plan_path_568 = os.path.join(pardir(568), 'nnUNetPlans_pretrained_POSTOPP_plans_3D.pkl')\n",
    "\n",
    "# Now the 569 stuff ---\n",
    "# The plan derived using the 568 P_plan\n",
    "POSTOPP_plan_path_569 = os.path.join(pardir(569), 'nnUNetPlans_pretrained_POSTOPP_plans_3D.pkl')\n",
    "# The original plan produced during independent preprocessing\n",
    "Orig_plan_path_569 = os.path.join(pardir(569), 'nnUNetPlansv2.1_plans_3D.pkl')\n",
    "# A placeholder path for holding the postopp plan while we overwrite it\n",
    "Backup_P_plan_path_569 = os.path.join(pardir(569), 'BACKUP_nnUNetPlans_pretrained_POSTOPP_plans_3D.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's see that the model architectures do look the same in each of the POSTOPP plans (568 versus 569) - and that it looks different in the 569 plan coming from its own preprocessing\n",
    "\n",
    "P_plan_568 = read_pkl(POSTOPP_plan_path_568)\n",
    "P_plan_569 = read_pkl(POSTOPP_plan_path_569)\n",
    "\n",
    "O_plan_569 = read_pkl(Orig_plan_path_569)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 2,\n",
       " 'num_pool_per_axis': [5, 5, 4],\n",
       " 'patch_size': array([128, 160, 112]),\n",
       " 'median_patient_size_in_voxels': array([137, 171, 130]),\n",
       " 'current_spacing': array([1., 1., 1.]),\n",
       " 'original_spacing': array([1., 1., 1.]),\n",
       " 'do_dummy_2D_data_aug': False,\n",
       " 'pool_op_kernel_sizes': [[2, 2, 2],\n",
       "  [2, 2, 2],\n",
       "  [2, 2, 2],\n",
       "  [2, 2, 2],\n",
       "  [2, 2, 1]],\n",
       " 'conv_kernel_sizes': [[3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_plan_568['plans_per_stage'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 2,\n",
       " 'num_pool_per_axis': [5, 5, 4],\n",
       " 'patch_size': array([128, 160, 112]),\n",
       " 'median_patient_size_in_voxels': array([137, 171, 130]),\n",
       " 'current_spacing': array([1., 1., 1.]),\n",
       " 'original_spacing': array([1., 1., 1.]),\n",
       " 'do_dummy_2D_data_aug': False,\n",
       " 'pool_op_kernel_sizes': [[2, 2, 2],\n",
       "  [2, 2, 2],\n",
       "  [2, 2, 2],\n",
       "  [2, 2, 2],\n",
       "  [2, 2, 1]],\n",
       " 'conv_kernel_sizes': [[3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_plan_569['plans_per_stage'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yay, they are the same\n",
    "\n",
    "# Now to see that the Original plan at 569 has a different model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 2,\n",
       " 'num_pool_per_axis': [5, 5, 5],\n",
       " 'patch_size': array([128, 128, 128]),\n",
       " 'median_patient_size_in_voxels': array([138, 161, 135]),\n",
       " 'current_spacing': array([1., 1., 1.]),\n",
       " 'original_spacing': array([1., 1., 1.]),\n",
       " 'do_dummy_2D_data_aug': False,\n",
       " 'pool_op_kernel_sizes': [[2, 2, 2],\n",
       "  [2, 2, 2],\n",
       "  [2, 2, 2],\n",
       "  [2, 2, 2],\n",
       "  [2, 2, 2]],\n",
       " 'conv_kernel_sizes': [[3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3],\n",
       "  [3, 3, 3]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_plan_569['plans_per_stage'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, note for one that the num_pool_per_axis are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetPlans_pretrained_POSTOPP_plans_3D.pkl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's see training fail if we drop in the original plan\n",
    "# first make a backup\n",
    "shutil.copyfile(src=POSTOPP_plan_path_569,dst=Backup_P_plan_path_569)\n",
    "shutil.copyfile(src=Orig_plan_path_569,dst=POSTOPP_plan_path_569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "Starting training for task: Task569_MultPathTest\n",
      "\n",
      "###############################################\n",
      "I am running the following nnUNet: 3d_fullres\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  4\n",
      "modalities:  {0: '_0000', 1: '_0001', 2: '_0002', 3: '_0003'}\n",
      "use_mask_for_norm OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'nonCT'), (1, 'nonCT'), (2, 'nonCT'), (3, 'nonCT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [5, 5, 5], 'patch_size': array([128, 128, 128]), 'median_patient_size_in_voxels': array([138, 161, 135]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using sample dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1\n",
      "###############################################\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2024-06-02 15:25:20.428326: Using splits from existing split file: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/splits_final.pkl\n",
      "2024-06-02 15:25:20.430270: The split file contains 5 splits.\n",
      "2024-06-02 15:25:20.430835: Desired fold for training: 0\n",
      "2024-06-02 15:25:20.431371: This split has 4 training and 1 validation cases.\n",
      "unpacking dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "2024-06-02 15:25:21.885410: loading checkpoint /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task569_MultPathTest/nnUNetTrainerV2__nnUNetPlans_pretrained_POSTOPP/fold_0/model_final_checkpoint.model train= True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generic_UNet:\n\tsize mismatch for tu.0.weight: copying a param with shape torch.Size([320, 320, 2, 2, 1]) from checkpoint, the shape in current model is torch.Size([320, 320, 2, 2, 2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now try to train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_on_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTask569_MultPathTest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mtrain_on_task\u001b[0;34m(task, continue_training, current_epoch, without_data_unpacking, use_compressed_data)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_on_task\u001b[39m(task, continue_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, current_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, without_data_unpacking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_compressed_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m###########\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training for task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mtrain_nnunet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnetwork_trainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnetwork_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcontinue_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinue_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_compressed_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compressed_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/be-SATGOpenFL/openfl/federated/task/nnunet_v1.py:243\u001b[0m, in \u001b[0;36mtrain_nnunet\u001b[0;34m(epochs, current_epoch, network, network_trainer, task, fold, continue_training, validation_only, c, p, use_compressed_data, deterministic, npz, find_lr, valbest, fp32, val_folder, disable_saving, disable_postprocessing_on_folds, val_disable_overwrite, disable_next_stage_pred, pretrained_weights)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validation_only:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;66;03m# -c was set, continue a previous training and ignore pretrained weights\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m         \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_latest_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcontinue_training) \u001b[38;5;129;01mand\u001b[39;00m (args\u001b[38;5;241m.\u001b[39mpretrained_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# we start a new training. If pretrained_weights are set, use them\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         load_pretrained_weights(trainer\u001b[38;5;241m.\u001b[39mnetwork, args\u001b[38;5;241m.\u001b[39mpretrained_weights)\n",
      "File \u001b[0;32m~/repositories/nnUNet/nnunet/training/network_training/network_trainer.py:302\u001b[0m, in \u001b[0;36mNetworkTrainer.load_latest_checkpoint\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_latest_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfile(join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_final_checkpoint.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_final_checkpoint.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfile(join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_latest.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_checkpoint(join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_latest.model\u001b[39m\u001b[38;5;124m\"\u001b[39m), train\u001b[38;5;241m=\u001b[39mtrain)\n",
      "File \u001b[0;32m~/repositories/nnUNet/nnunet/training/network_training/network_trainer.py:321\u001b[0m, in \u001b[0;36mNetworkTrainer.load_checkpoint\u001b[0;34m(self, fname, train)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# saved_model = torch.load(fname, map_location=torch.device('cuda', torch.cuda.current_device()))\u001b[39;00m\n\u001b[1;32m    320\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(fname, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 321\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint_ram\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/nnUNet/nnunet/training/network_training/network_trainer.py:365\u001b[0m, in \u001b[0;36mNetworkTrainer.load_checkpoint_ram\u001b[0;34m(self, checkpoint, train)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamp_grad_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp_grad_scaler\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamp_grad_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "File \u001b[0;32m~/virtual/be-SATGOpenFL-too/lib/python3.8/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generic_UNet:\n\tsize mismatch for tu.0.weight: copying a param with shape torch.Size([320, 320, 2, 2, 1]) from checkpoint, the shape in current model is torch.Size([320, 320, 2, 2, 2])."
     ]
    }
   ],
   "source": [
    "# Now try to train\n",
    "train_on_task(task='Task569_MultPathTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetPlans_pretrained_POSTOPP_plans_3D.pkl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now restore the plan and try training again\n",
    "shutil.copyfile(src=Backup_P_plan_path_569,dst=POSTOPP_plan_path_569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "Starting training for task: Task569_MultPathTest\n",
      "\n",
      "###############################################\n",
      "I am running the following nnUNet: 3d_fullres\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  4\n",
      "modalities:  {0: '_0000', 1: '_0001', 2: '_0002', 3: '_0003'}\n",
      "use_mask_for_norm OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'nonCT'), (1, 'nonCT'), (2, 'nonCT'), (3, 'nonCT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [5, 5, 4], 'patch_size': array([128, 160, 112]), 'median_patient_size_in_voxels': array([137, 171, 130]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using sample dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1\n",
      "###############################################\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2024-06-02 15:27:07.713723: Using splits from existing split file: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/splits_final.pkl\n",
      "2024-06-02 15:27:07.716092: The split file contains 5 splits.\n",
      "2024-06-02 15:27:07.716663: Desired fold for training: 0\n",
      "2024-06-02 15:27:07.717211: This split has 4 training and 1 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "2024-06-02 15:27:08.356620: loading checkpoint /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task569_MultPathTest/nnUNetTrainerV2__nnUNetPlans_pretrained_POSTOPP/fold_0/model_final_checkpoint.model train= True\n",
      "2024-06-02 15:27:08.556991: lr: 0.009982\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-06-02 15:27:20.754251: Unable to plot network architecture:\n",
      "2024-06-02 15:27:20.755180: No module named 'hiddenlayer'\n",
      "2024-06-02 15:27:20.768709: \n",
      "printing the network instead:\n",
      "\n",
      "2024-06-02 15:27:20.769321: Generic_UNet(\n",
      "  (conv_blocks_localization): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_blocks_context): ModuleList(\n",
      "    (0): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (td): ModuleList()\n",
      "  (tu): ModuleList(\n",
      "    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 1), stride=(2, 2, 1), bias=False)\n",
      "    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "  )\n",
      "  (seg_outputs): ModuleList(\n",
      "    (0): Conv3d(320, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): Conv3d(256, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (2): Conv3d(128, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (3): Conv3d(64, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (4): Conv3d(32, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "2024-06-02 15:27:20.777075: \n",
      "\n",
      "2024-06-02 15:27:21.078755: saving checkpoint to /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task569_MultPathTest/nnUNetTrainerV2__nnUNetPlans_pretrained_POSTOPP/fold_0/model_final_checkpoint.model...\n",
      "2024-06-02 15:27:22.109070: done, saving took 1.33 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now try to train\n",
    "train_on_task(task='Task569_MultPathTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Refining test below to make it cleaner\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "sys.path.append('/home/edwardsb/repositories/be-SATGOpenFL/openfl/federated/task')\n",
    "# older script\n",
    "from fedsim_setup import main as setup_fedsim\n",
    "from fl_setup import main as setup_fl\n",
    "from nnunet_v1_with_old_plans_name_hardcoded import train_nnunet\n",
    "from nnunet.paths import default_plans_identifier\n",
    "\n",
    "network = '3d_fullres'\n",
    "network_trainer = 'nnUNetTrainerV2'\n",
    "fold = '0'\n",
    "\n",
    "cuda_device='5'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=cuda_device\n",
    "\n",
    "def train_on_task(task, continue_training=True, current_epoch=0, without_data_unpacking=False, use_compressed_data=False):\n",
    "    print(f\"###########\\nStarting training for task: {task}\\n\")\n",
    "    train_nnunet(epochs=1, \n",
    "                 current_epoch = current_epoch, \n",
    "                 network = network,\n",
    "                 task=task, \n",
    "                 network_trainer = network_trainer, \n",
    "                 fold=fold, \n",
    "                 continue_training=continue_training, \n",
    "                 use_compressed_data=use_compressed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_tree(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######### CREATING SYMLINKS TO POSTOPP DATA FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "######### GENERATING DATA JSON FILE FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "######### OS CALL TO PREPROCESS DATA FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "Verifying training set\n",
      "checking case BraTS2021_00468_2008.03.26\n",
      "checking case BraTS2021_00512_2008.03.26\n",
      "checking case BraTS2021_00512_2008.12.11\n",
      "checking case BraTS2021_00565_2008.03.26\n",
      "checking case BraTS2021_01277_2008.03.26\n",
      "Verifying label values\n",
      "Expected label values are [0, 1, 2, 3, 4]\n",
      "Labels OK\n",
      "Dataset OK\n",
      "BraTS2021_00468_2008.03.26\n",
      "BraTS2021_00512_2008.03.26\n",
      "BraTS2021_00512_2008.12.11\n",
      "BraTS2021_00565_2008.03.26\n",
      "BraTS2021_01277_2008.03.26\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 141, 176, 141) spacing: [1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 137, 173, 128) spacing: [1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 136, 171, 130) spacing: before crop: (4, 155, 240, 240) after crop: (4, 136, 171, 130) spacing: [1. 1. 1.] \n",
      "\n",
      "[1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 144, 169, 133) spacing: [1. 1. 1.] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Task568_MultPathTest\n",
      "number of threads:  (8, 8) \n",
      "\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "Are we using the nonzero mask for normalization? OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "the median shape of the dataset is  [137. 171. 130.]\n",
      "the max shape in the dataset is  [144. 176. 141.]\n",
      "the min shape in the dataset is  [136. 169. 128.]\n",
      "we don't want feature maps smaller than  4  in the bottleneck\n",
      "the transposed median shape of the dataset is  [137. 171. 130.]\n",
      "generating configuration for 3d_fullres\n",
      "{0: {'batch_size': 2, 'num_pool_per_axis': [5, 5, 4], 'patch_size': array([128, 160, 112]), 'median_patient_size_in_voxels': array([137, 171, 130]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}}\n",
      "transpose forward [0, 1, 2]\n",
      "transpose backward [0, 1, 2]\n",
      "Initializing to run preprocessing\n",
      "npz folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_cropped_data/Task568_MultPathTest\n",
      "output_folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 137, 173, 128)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 137, 173, 128)} \n",
      "\n",
      "{'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 136, 171, 130)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 136, 171, 130)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 136, 171, 130)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 136, 171, 130)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 141, 176, 141)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 141, 176, 141)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 144, 169, 133)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 144, 169, 133)} \n",
      "\n",
      "1 10000\n",
      "1 8106\n",
      "1 10000\n",
      "2 10000\n",
      "2 10000\n",
      "2 10000\n",
      "1 10000\n",
      "1 3391\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_00565_2008.03.26.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_00512_2008.12.11.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_00512_2008.03.26.npz\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_00468_2008.03.26.npz\n",
      "4 7167\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_01277_2008.03.26.npz\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "Are we using the nonzero mask for normalization? OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "the median shape of the dataset is  [137. 171. 130.]\n",
      "the max shape in the dataset is  [144. 176. 141.]\n",
      "the min shape in the dataset is  [136. 169. 128.]\n",
      "we don't want feature maps smaller than  4  in the bottleneck\n",
      "the transposed median shape of the dataset is  [137. 171. 130.]\n",
      "[{'batch_size': 25, 'num_pool_per_axis': [5, 5], 'patch_size': array([192, 160]), 'median_patient_size_in_voxels': array([137, 171, 130]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}]\n",
      "Initializing to run preprocessing\n",
      "npz folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_cropped_data/Task568_MultPathTest\n",
      "output_folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 137, 173, 128)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 137, 173, 128)} \n",
      "\n",
      "normalization...\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 136, 171, 130)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 136, 171, 130)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 136, 171, 130)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 136, 171, 130)} \n",
      "\n",
      "normalization...\n",
      "normalization...\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 141, 176, 141)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 141, 176, 141)} \n",
      "\n",
      "normalization...\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 144, 169, 133)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 144, 169, 133)} \n",
      "\n",
      "normalization...\n",
      "normalization done\n",
      "normalization done\n",
      "normalization done\n",
      "normalization done\n",
      "1 8106\n",
      "1 10000\n",
      "normalization done\n",
      "1 10000\n",
      "2 10000\n",
      "2 10000\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_00565_2008.03.26.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_00512_2008.12.11.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_00512_2008.03.26.npz\n",
      "1 3391\n",
      "1 10000\n",
      "2 10000\n",
      "2 10000\n",
      "4 7167\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_01277_2008.03.26.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_00468_2008.03.26.npz\n",
      "\n",
      "###########\n",
      "Deleting 2D data directory at: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0 \n",
      "##############\n",
      "\n",
      "###########\n",
      "Starting training for task: Task568_MultPathTest\n",
      "\n",
      "###############################################\n",
      "I am running the following nnUNet: 3d_fullres\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  4\n",
      "modalities:  {0: '_0000', 1: '_0001', 2: '_0002', 3: '_0003'}\n",
      "use_mask_for_norm OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'nonCT'), (1, 'nonCT'), (2, 'nonCT'), (3, 'nonCT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [5, 5, 4], 'patch_size': array([128, 160, 112]), 'median_patient_size_in_voxels': array([137, 171, 130]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using sample dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1\n",
      "###############################################\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2024-06-03 11:38:42.012005: Creating new 5-fold cross-validation split...\n",
      "2024-06-03 11:38:42.015541: Desired fold for training: 0\n",
      "2024-06-03 11:38:42.016019: This split has 4 training and 1 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "2024-06-03 11:38:44.438717: lr: 0.01\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-06-03 11:38:47.493761: Unable to plot network architecture:\n",
      "2024-06-03 11:38:47.494903: No module named 'hiddenlayer'\n",
      "2024-06-03 11:38:47.495668: \n",
      "printing the network instead:\n",
      "\n",
      "2024-06-03 11:38:47.496170: Generic_UNet(\n",
      "  (conv_blocks_localization): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_blocks_context): ModuleList(\n",
      "    (0): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (td): ModuleList()\n",
      "  (tu): ModuleList(\n",
      "    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 1), stride=(2, 2, 1), bias=False)\n",
      "    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "  )\n",
      "  (seg_outputs): ModuleList(\n",
      "    (0): Conv3d(320, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): Conv3d(256, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (2): Conv3d(128, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (3): Conv3d(64, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (4): Conv3d(32, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "2024-06-03 11:38:47.515238: \n",
      "\n",
      "2024-06-03 11:38:47.516168: \n",
      "epoch:  0\n",
      "2024-06-03 11:46:04.023653: train loss : -0.2572\n",
      "2024-06-03 11:46:24.121329: validation loss: -0.1282\n",
      "2024-06-03 11:46:24.124721: Average global foreground Dice: [0.3347, 0.5543, 0.6909]\n",
      "2024-06-03 11:46:24.125526: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edwardsb/repositories/nnUNet/nnunet/training/network_training/nnUNetTrainer.py:719: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  global_dc_per_class = [i for i in [2 * i / (2 * i + j + k) for i, j, k in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 11:46:24.502413: lr: 0.009991\n",
      "2024-06-03 11:46:24.503276: This epoch took 456.986573 s\n",
      "\n",
      "2024-06-03 11:46:24.578341: saving checkpoint to /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task568_MultPathTest/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model...\n",
      "2024-06-03 11:46:25.086878: done, saving took 0.58 seconds\n",
      "\n",
      "######### CREATING SYMLINKS TO POSTOPP DATA FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "######### GENERATING DATA JSON FILE FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "######### OS CALL TO PREPROCESS DATA FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "Verifying training set\n",
      "checking case BraTS2021_00122_2008.03.26\n",
      "checking case BraTS2021_01200_2008.03.26\n",
      "checking case BraTS2021_01200_2008.12.11\n",
      "checking case BraTS2021_01299_2008.03.26\n",
      "checking case BraTS2021_01394_2008.03.26\n",
      "Verifying label values\n",
      "Expected label values are [0, 1, 2, 3, 4]\n",
      "Labels OK\n",
      "Dataset OK\n",
      "BraTS2021_00122_2008.03.26\n",
      "BraTS2021_01200_2008.03.26\n",
      "BraTS2021_01200_2008.12.11\n",
      "BraTS2021_01299_2008.03.26\n",
      "BraTS2021_01394_2008.03.26\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 139, 161, 136) spacing: [1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 142, 162, 135) spacing: before crop: (4, 155, 240, 240) after crop: (4, 130, 159, 123) spacing: [1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 138, 168, 135) spacing: [1. 1. 1.] \n",
      "\n",
      "[1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 130, 159, 123) spacing: [1. 1. 1.] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Task569_MultPathTest\n",
      "number of threads:  (8, 8) \n",
      "\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "Are we using the nonzero mask for normalization? OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "the median shape of the dataset is  [138. 161. 135.]\n",
      "the max shape in the dataset is  [142. 168. 136.]\n",
      "the min shape in the dataset is  [130. 159. 123.]\n",
      "we don't want feature maps smaller than  4  in the bottleneck\n",
      "the transposed median shape of the dataset is  [138. 161. 135.]\n",
      "generating configuration for 3d_fullres\n",
      "{0: {'batch_size': 2, 'num_pool_per_axis': [5, 5, 5], 'patch_size': array([128, 128, 128]), 'median_patient_size_in_voxels': array([138, 161, 135]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}}\n",
      "transpose forward [0, 1, 2]\n",
      "transpose backward [0, 1, 2]\n",
      "Initializing to run preprocessing\n",
      "npz folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_cropped_data/Task569_MultPathTest\n",
      "output_folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest\n",
      "no resampling necessaryno resampling necessary\n",
      "\n",
      "no resampling necessaryno resampling necessary\n",
      "\n",
      "before:before:  {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 130, 159, 123)}{'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 130, 159, 123)}  \n",
      "after: \n",
      "after:   {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 130, 159, 123)} {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 130, 159, 123)}\n",
      " \n",
      "\n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 142, 162, 135)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 142, 162, 135)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 138, 168, 135)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 138, 168, 135)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 139, 161, 136)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 139, 161, 136)} \n",
      "\n",
      "1 10000\n",
      "1 10000\n",
      "2 10000\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_01200_2008.12.11.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_01200_2008.03.26.npz\n",
      "1 6056\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_01299_2008.03.26.npz\n",
      "1 317\n",
      "2 10000\n",
      "1 10000\n",
      "2 10000\n",
      "4 4409\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_01394_2008.03.26.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_00122_2008.03.26.npz\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "Are we using the nonzero mask for normalization? OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "the median shape of the dataset is  [138. 161. 135.]\n",
      "the max shape in the dataset is  [142. 168. 136.]\n",
      "the min shape in the dataset is  [130. 159. 123.]\n",
      "we don't want feature maps smaller than  4  in the bottleneck\n",
      "the transposed median shape of the dataset is  [138. 161. 135.]\n",
      "[{'batch_size': 24, 'num_pool_per_axis': [5, 5], 'patch_size': array([192, 160]), 'median_patient_size_in_voxels': array([138, 161, 135]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}]\n",
      "Initializing to run preprocessing\n",
      "npz folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_cropped_data/Task569_MultPathTest\n",
      "output_folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 130, 159, 123)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 130, 159, 123)} \n",
      "\n",
      "normalization...\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 139, 161, 136)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 139, 161, 136)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 142, 162, 135)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 142, 162, 135)} \n",
      "\n",
      "normalization...\n",
      "normalization...\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 130, 159, 123)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 130, 159, 123)} \n",
      "\n",
      "normalization...\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 138, 168, 135)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 138, 168, 135)} \n",
      "\n",
      "normalization...\n",
      "normalization done\n",
      "normalization done\n",
      "normalization done\n",
      "normalization done\n",
      "1 10000\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_01200_2008.12.11.npz\n",
      "normalization done\n",
      "1 10000\n",
      "2 10000\n",
      "1 6056\n",
      "1 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_01200_2008.03.26.npz\n",
      "2 10000\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_00122_2008.03.26.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_01299_2008.03.26.npz\n",
      "1 317\n",
      "2 10000\n",
      "4 4409\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_01394_2008.03.26.npz\n",
      "\n",
      "###########\n",
      "Deleting 2D data directory at: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0 \n",
      "##############\n",
      "\n",
      "\n",
      "######### COPYING INITIAL MODEL FILES INTO COLLABORATOR 0 FOLDERS #########\n",
      "\n",
      "###########\n",
      "Starting training for task: Task569_MultPathTest\n",
      "\n",
      "###############################################\n",
      "I am running the following nnUNet: 3d_fullres\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  4\n",
      "modalities:  {0: '_0000', 1: '_0001', 2: '_0002', 3: '_0003'}\n",
      "use_mask_for_norm OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'nonCT'), (1, 'nonCT'), (2, 'nonCT'), (3, 'nonCT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [5, 5, 5], 'patch_size': array([128, 128, 128]), 'median_patient_size_in_voxels': array([138, 161, 135]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using sample dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1\n",
      "###############################################\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2024-06-03 11:46:38.803222: Using splits from existing split file: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/splits_final.pkl\n",
      "2024-06-03 11:46:38.805384: The split file contains 5 splits.\n",
      "2024-06-03 11:46:38.806000: Desired fold for training: 0\n",
      "2024-06-03 11:46:38.806599: This split has 4 training and 1 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "2024-06-03 11:46:40.413557: loading checkpoint /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task569_MultPathTest/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generic_UNet:\n\tsize mismatch for tu.0.weight: copying a param with shape torch.Size([320, 320, 2, 2, 1]) from checkpoint, the shape in current model is torch.Size([320, 320, 2, 2, 2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 81\u001b[0m\n\u001b[1;32m     65\u001b[0m setup_fedsim(postopp_pardirs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/raid/edwardsb/projects/RANO/test_data_links_random_times_1\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     66\u001b[0m          first_three_digit_task_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m569\u001b[39m,\n\u001b[1;32m     67\u001b[0m          init_model_path\u001b[38;5;241m=\u001b[39minit_model_path_568,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m          cuda_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     78\u001b[0m          verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Now try to train and see we get a shape mismatch (note I need to replace the new default value for p below as before we used the default plans identifier but now the code useds a POSTOPP specific string)\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mtrain_on_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTask569_MultPathTest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mtrain_on_task\u001b[0;34m(task, continue_training, current_epoch, without_data_unpacking, use_compressed_data)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_on_task\u001b[39m(task, continue_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, current_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, without_data_unpacking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_compressed_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m###########\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training for task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mtrain_nnunet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnetwork_trainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnetwork_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcontinue_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinue_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_compressed_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compressed_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/be-SATGOpenFL/openfl/federated/task/nnunet_v1_with_old_plans_name_hardcoded.py:243\u001b[0m, in \u001b[0;36mtrain_nnunet\u001b[0;34m(epochs, current_epoch, network, network_trainer, task, fold, continue_training, validation_only, c, p, use_compressed_data, deterministic, npz, find_lr, valbest, fp32, val_folder, disable_saving, disable_postprocessing_on_folds, val_disable_overwrite, disable_next_stage_pred, pretrained_weights)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validation_only:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;66;03m# -c was set, continue a previous training and ignore pretrained weights\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m         \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_latest_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcontinue_training) \u001b[38;5;129;01mand\u001b[39;00m (args\u001b[38;5;241m.\u001b[39mpretrained_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# we start a new training. If pretrained_weights are set, use them\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         load_pretrained_weights(trainer\u001b[38;5;241m.\u001b[39mnetwork, args\u001b[38;5;241m.\u001b[39mpretrained_weights)\n",
      "File \u001b[0;32m~/repositories/nnUNet/nnunet/training/network_training/network_trainer.py:302\u001b[0m, in \u001b[0;36mNetworkTrainer.load_latest_checkpoint\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_latest_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfile(join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_final_checkpoint.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_final_checkpoint.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfile(join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_latest.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_checkpoint(join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_latest.model\u001b[39m\u001b[38;5;124m\"\u001b[39m), train\u001b[38;5;241m=\u001b[39mtrain)\n",
      "File \u001b[0;32m~/repositories/nnUNet/nnunet/training/network_training/network_trainer.py:321\u001b[0m, in \u001b[0;36mNetworkTrainer.load_checkpoint\u001b[0;34m(self, fname, train)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# saved_model = torch.load(fname, map_location=torch.device('cuda', torch.cuda.current_device()))\u001b[39;00m\n\u001b[1;32m    320\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(fname, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 321\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint_ram\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/nnUNet/nnunet/training/network_training/network_trainer.py:365\u001b[0m, in \u001b[0;36mNetworkTrainer.load_checkpoint_ram\u001b[0;34m(self, checkpoint, train)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamp_grad_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp_grad_scaler\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamp_grad_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "File \u001b[0;32m~/virtual/be-SATGOpenFL-too/lib/python3.8/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generic_UNet:\n\tsize mismatch for tu.0.weight: copying a param with shape torch.Size([320, 320, 2, 2, 1]) from checkpoint, the shape in current model is torch.Size([320, 320, 2, 2, 2])."
     ]
    }
   ],
   "source": [
    "# NOTE: The training runs included in the test below take about 6 minutes on spr01 (Titan XP)\n",
    "\n",
    "# First do what we did before\n",
    "\n",
    "# Some paths below will be under my folders, but someone else might want to replace this part\n",
    "test_pardir = '/raid/edwardsb/projects/RANO'\n",
    "\n",
    "\n",
    "datadir_568 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_preprocessed', 'Task568_MultPathTest')\n",
    "# next two for deleting only\n",
    "alt_1_datadir_568 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_raw_data', 'Task568_MultPathTest')\n",
    "alt_2_datadir_568 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_cropped_data', 'Task568_MultPathTest')\n",
    "\n",
    "model_pre_dir_568 = os.path.join(test_pardir, 'NNUnetModels', 'nnUNet', '3d_fullres', 'Task568_MultPathTest')\n",
    "modeldir_568 = os.path.join(model_pre_dir_568, 'nnUNetTrainerV2__nnUNetPlansv2.1', 'fold_0')\n",
    "\n",
    "datadir_569 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_preprocessed', 'Task569_MultPathTest')\n",
    "# next two for deleting only\n",
    "alt_1_datadir_569 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_raw_data', 'Task569_MultPathTest')\n",
    "alt_2_datadir_569 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_cropped_data', 'Task569_MultPathTest')\n",
    "\n",
    "model_pre_dir_569 = os.path.join(test_pardir, 'NNUnetModels', 'nnUNet', '3d_fullres', 'Task569_MultPathTest')\n",
    "model_dir_569 = os.path.join(model_pre_dir_569, 'nnUNetTrainerV2__nnUNetPlansv2.1', 'fold_0')\n",
    "\n",
    "# At this point deleted some folders via linux command line\n",
    "\n",
    "rm_tree(datadir_568)\n",
    "rm_tree(alt_1_datadir_568)\n",
    "rm_tree(alt_2_datadir_568)\n",
    "rm_tree(model_pre_dir_568)\n",
    "\n",
    "\n",
    "rm_tree(datadir_569)\n",
    "rm_tree(alt_1_datadir_569)\n",
    "rm_tree(alt_2_datadir_569)\n",
    "rm_tree(model_pre_dir_569)\n",
    "\n",
    "\n",
    "setup_fedsim(postopp_pardirs=['/raid/edwardsb/projects/RANO/test_data_links_random_times_0'], \n",
    "         first_three_digit_task_num=568,\n",
    "         init_model_path=None,\n",
    "         init_model_info_path= None, \n",
    "         task_name='MultPathTest', \n",
    "         percent_train=0.8, \n",
    "         split_logic='by_subject_time_pair', \n",
    "         network='3d_fullres', \n",
    "         network_trainer='nnUNetTrainerV2', \n",
    "         fold=0,  \n",
    "         timestamp_selection='all', \n",
    "         num_institutions=1, \n",
    "         cuda_device='5', \n",
    "         verbose=False)\n",
    "\n",
    "# The source for the init model paths are located in the 568 results folder\n",
    "init_model_path_568 = os.path.join(modeldir_568, 'model_initial_checkpoint.model') \n",
    "init_model_info_path_568 =  os.path.join(modeldir_568, 'model_initial_checkpoint.model.pkl') \n",
    "\n",
    "# The plan paths are under the data folders\n",
    "plan_path_568 = os.path.join(datadir_568, 'nnUNetPlansv2.1_plans_3D.pkl')\n",
    "plan_path_569 = os.path.join(datadir_569, 'nnUNetPlansv2.1_plans_3D.pkl')\n",
    "\n",
    "# Now set up 569 as we did in the first fed test, with the old script that passes only initial model and initial model info bu only shares plan across the first collaborator\n",
    "# in the call with the others (though there are no others here)\n",
    "\n",
    "setup_fedsim(postopp_pardirs=['/raid/edwardsb/projects/RANO/test_data_links_random_times_1'], \n",
    "         first_three_digit_task_num=569,\n",
    "         init_model_path=init_model_path_568,\n",
    "         init_model_info_path= init_model_info_path_568,\n",
    "         task_name='MultPathTest', \n",
    "         percent_train=0.8, \n",
    "         split_logic='by_subject_time_pair', \n",
    "         network='3d_fullres', \n",
    "         network_trainer='nnUNetTrainerV2', \n",
    "         fold=0,  \n",
    "         timestamp_selection='all', \n",
    "         num_institutions=1, \n",
    "         cuda_device='5', \n",
    "         verbose=False)\n",
    "\n",
    "# Now try to train and see we get a shape mismatch (note I need to replace the new default value for p below as before we used the default plans identifier but now the code useds a POSTOPP specific string)\n",
    "train_on_task(task='Task569_MultPathTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetPlansv2.1_plans_3D.pkl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now drop in the plans file that should have been shared and see that training works now\n",
    "\n",
    "shutil.copyfile(src=plan_path_568,dst=plan_path_569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "Starting training for task: Task569_MultPathTest\n",
      "\n",
      "###############################################\n",
      "I am running the following nnUNet: 3d_fullres\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  4\n",
      "modalities:  {0: '_0000', 1: '_0001', 2: '_0002', 3: '_0003'}\n",
      "use_mask_for_norm OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'nonCT'), (1, 'nonCT'), (2, 'nonCT'), (3, 'nonCT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [5, 5, 4], 'patch_size': array([128, 160, 112]), 'median_patient_size_in_voxels': array([137, 171, 130]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using sample dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1\n",
      "###############################################\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2024-06-03 11:46:52.499521: Using splits from existing split file: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/splits_final.pkl\n",
      "2024-06-03 11:46:52.502803: The split file contains 5 splits.\n",
      "2024-06-03 11:46:52.503469: Desired fold for training: 0\n",
      "2024-06-03 11:46:52.504105: This split has 4 training and 1 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "2024-06-03 11:46:53.296526: loading checkpoint /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task569_MultPathTest/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True\n",
      "2024-06-03 11:46:53.478378: lr: 0.009991\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-06-03 11:47:05.040187: Unable to plot network architecture:\n",
      "2024-06-03 11:47:05.052979: No module named 'hiddenlayer'\n",
      "2024-06-03 11:47:05.053844: \n",
      "printing the network instead:\n",
      "\n",
      "2024-06-03 11:47:05.054426: Generic_UNet(\n",
      "  (conv_blocks_localization): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_blocks_context): ModuleList(\n",
      "    (0): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (td): ModuleList()\n",
      "  (tu): ModuleList(\n",
      "    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 1), stride=(2, 2, 1), bias=False)\n",
      "    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "  )\n",
      "  (seg_outputs): ModuleList(\n",
      "    (0): Conv3d(320, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): Conv3d(256, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (2): Conv3d(128, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (3): Conv3d(64, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (4): Conv3d(32, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "2024-06-03 11:47:05.058112: \n",
      "\n",
      "2024-06-03 11:47:05.241560: saving checkpoint to /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task569_MultPathTest/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model...\n",
      "2024-06-03 11:47:05.887483: done, saving took 0.83 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now try to train again and this time see it works\n",
    "train_on_task(task='Task569_MultPathTest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "be-SATGOpenFL-too",
   "language": "python",
   "name": "be-satgopenfl-too"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
