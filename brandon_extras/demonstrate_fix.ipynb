{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this notebook with a virtual environment that has NNUnet V1 installed\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "sys.path.append('/home/edwardsb/repositories/be-SATGOpenFL/openfl/federated/task')\n",
    "# older script\n",
    "from fedsim_setup import main as setup_fedsim\n",
    "from fl_setup import main as setup_fl\n",
    "from nnunet_v1_with_old_plans_name_hardcoded import train_nnunet\n",
    "from nnunet.paths import default_plans_identifier\n",
    "\n",
    "network = '3d_fullres'\n",
    "network_trainer = 'nnUNetTrainerV2'\n",
    "fold = '0'\n",
    "\n",
    "cuda_device='5'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=cuda_device\n",
    "\n",
    "def train_on_task(task, continue_training=True, current_epoch=0, without_data_unpacking=False, use_compressed_data=False):\n",
    "    print(f\"###########\\nStarting training for task: {task}\\n\")\n",
    "    train_nnunet(epochs=1, \n",
    "                 current_epoch = current_epoch, \n",
    "                 network = network,\n",
    "                 task=task, \n",
    "                 network_trainer = network_trainer, \n",
    "                 fold=fold, \n",
    "                 continue_training=continue_training, \n",
    "                 use_compressed_data=use_compressed_data)\n",
    "    \n",
    "def rm_tree(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######### CREATING SYMLINKS TO POSTOPP DATA FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "######### GENERATING DATA JSON FILE FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "######### OS CALL TO PREPROCESS DATA FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "Verifying training set\n",
      "checking case BraTS2021_00468_2008.03.26\n",
      "checking case BraTS2021_00512_2008.03.26\n",
      "checking case BraTS2021_00512_2008.12.11\n",
      "checking case BraTS2021_00565_2008.03.26\n",
      "checking case BraTS2021_01277_2008.03.26\n",
      "Verifying label values\n",
      "Expected label values are [0, 1, 2, 3, 4]\n",
      "Labels OK\n",
      "Dataset OK\n",
      "BraTS2021_00468_2008.03.26\n",
      "BraTS2021_00512_2008.03.26\n",
      "BraTS2021_00512_2008.12.11\n",
      "BraTS2021_00565_2008.03.26\n",
      "BraTS2021_01277_2008.03.26\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 137, 173, 128) spacing: [1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 144, 169, 133) spacing: [1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 141, 176, 141) spacing: [1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 136, 171, 130) spacing: [1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 136, 171, 130) spacing: [1. 1. 1.] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Task568_MultPathTest\n",
      "number of threads:  (8, 8) \n",
      "\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "Are we using the nonzero mask for normalization? OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "the median shape of the dataset is  [137. 171. 130.]\n",
      "the max shape in the dataset is  [144. 176. 141.]\n",
      "the min shape in the dataset is  [136. 169. 128.]\n",
      "we don't want feature maps smaller than  4  in the bottleneck\n",
      "the transposed median shape of the dataset is  [137. 171. 130.]\n",
      "generating configuration for 3d_fullres\n",
      "{0: {'batch_size': 2, 'num_pool_per_axis': [5, 5, 4], 'patch_size': array([128, 160, 112]), 'median_patient_size_in_voxels': array([137, 171, 130]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}}\n",
      "transpose forward [0, 1, 2]\n",
      "transpose backward [0, 1, 2]\n",
      "Initializing to run preprocessing\n",
      "npz folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_cropped_data/Task568_MultPathTest\n",
      "output_folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 136, 171, 130)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 136, 171, 130)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 137, 173, 128)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 137, 173, 128)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 136, 171, 130)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 136, 171, 130)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 144, 169, 133)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 144, 169, 133)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 141, 176, 141)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 141, 176, 141)} \n",
      "\n",
      "1 10000\n",
      "1 10000\n",
      "1 8106\n",
      "2 10000\n",
      "2 10000\n",
      "2 10000\n",
      "4 10000\n",
      "1saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_00512_2008.03.26.npz\n",
      " 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_00565_2008.03.26.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_00512_2008.12.11.npz\n",
      "2 10000\n",
      "1 3391\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_00468_2008.03.26.npz\n",
      "4 7167\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_01277_2008.03.26.npz\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "Are we using the nonzero mask for normalization? OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "the median shape of the dataset is  [137. 171. 130.]\n",
      "the max shape in the dataset is  [144. 176. 141.]\n",
      "the min shape in the dataset is  [136. 169. 128.]\n",
      "we don't want feature maps smaller than  4  in the bottleneck\n",
      "the transposed median shape of the dataset is  [137. 171. 130.]\n",
      "[{'batch_size': 25, 'num_pool_per_axis': [5, 5], 'patch_size': array([192, 160]), 'median_patient_size_in_voxels': array([137, 171, 130]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}]\n",
      "Initializing to run preprocessing\n",
      "npz folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_cropped_data/Task568_MultPathTest\n",
      "output_folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 136, 171, 130)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 136, 171, 130)} \n",
      "\n",
      "normalization...\n",
      "no resampling necessaryno resampling necessary\n",
      "\n",
      "no resampling necessaryno resampling necessary\n",
      "\n",
      "before:before:  {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 136, 171, 130)}{'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 137, 173, 128)}  \n",
      "after: \n",
      "after:   {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 137, 173, 128)}{'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 136, 171, 130)}  \n",
      "\n",
      "\n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 144, 169, 133)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 144, 169, 133)} \n",
      "\n",
      "normalization...\n",
      "normalization...\n",
      "normalization...\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 141, 176, 141)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 141, 176, 141)} \n",
      "\n",
      "normalization...\n",
      "normalization done\n",
      "normalization done\n",
      "normalization done\n",
      "normalization done\n",
      "normalization done\n",
      "1 10000\n",
      "1 10000\n",
      "1 8106\n",
      "2 10000\n",
      "1 10000\n",
      "2 10000\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_00512_2008.03.26.npz\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_00565_2008.03.26.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_00512_2008.12.11.npz\n",
      "1 3391\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_00468_2008.03.26.npz\n",
      "2 10000\n",
      "4 7167\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_01277_2008.03.26.npz\n",
      "\n",
      "###########\n",
      "Deleting 2D data directory at: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1_2D_stage0 \n",
      "##############\n",
      "\n",
      "###########\n",
      "Starting training for task: Task568_MultPathTest\n",
      "\n",
      "###############################################\n",
      "I am running the following nnUNet: 3d_fullres\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  4\n",
      "modalities:  {0: '_0000', 1: '_0001', 2: '_0002', 3: '_0003'}\n",
      "use_mask_for_norm OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'nonCT'), (1, 'nonCT'), (2, 'nonCT'), (3, 'nonCT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [5, 5, 4], 'patch_size': array([128, 160, 112]), 'median_patient_size_in_voxels': array([137, 171, 130]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using sample dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task568_MultPathTest/nnUNetData_plans_v2.1\n",
      "###############################################\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2024-06-03 11:50:58.400362: Creating new 5-fold cross-validation split...\n",
      "2024-06-03 11:50:58.404088: Desired fold for training: 0\n",
      "2024-06-03 11:50:58.404684: This split has 4 training and 1 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "2024-06-03 11:51:00.795704: lr: 0.01\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-06-03 11:51:10.249979: Unable to plot network architecture:\n",
      "2024-06-03 11:51:10.250949: No module named 'hiddenlayer'\n",
      "2024-06-03 11:51:10.251509: \n",
      "printing the network instead:\n",
      "\n",
      "2024-06-03 11:51:10.252035: Generic_UNet(\n",
      "  (conv_blocks_localization): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_blocks_context): ModuleList(\n",
      "    (0): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (td): ModuleList()\n",
      "  (tu): ModuleList(\n",
      "    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 1), stride=(2, 2, 1), bias=False)\n",
      "    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "  )\n",
      "  (seg_outputs): ModuleList(\n",
      "    (0): Conv3d(320, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): Conv3d(256, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (2): Conv3d(128, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (3): Conv3d(64, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (4): Conv3d(32, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "2024-06-03 11:51:10.255807: \n",
      "\n",
      "2024-06-03 11:51:10.256619: \n",
      "epoch:  0\n",
      "2024-06-03 11:58:17.562040: train loss : -0.3303\n",
      "2024-06-03 11:58:37.824046: validation loss: -0.1116\n",
      "2024-06-03 11:58:37.826030: Average global foreground Dice: [0.2644, 0.5732, 0.0, 0.697]\n",
      "2024-06-03 11:58:37.827708: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2024-06-03 11:58:38.219537: lr: 0.009991\n",
      "2024-06-03 11:58:38.220520: This epoch took 447.963332 s\n",
      "\n",
      "2024-06-03 11:58:38.294949: saving checkpoint to /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task568_MultPathTest/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model...\n",
      "2024-06-03 11:58:38.794963: done, saving took 0.57 seconds\n",
      "\n",
      "######### CREATING SYMLINKS TO POSTOPP DATA FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "######### GENERATING DATA JSON FILE FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "######### OS CALL TO PREPROCESS DATA FOR COLLABORATOR 0 #########\n",
      "\n",
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "Verifying training set\n",
      "checking case BraTS2021_00122_2008.03.26\n",
      "checking case BraTS2021_01200_2008.03.26\n",
      "checking case BraTS2021_01200_2008.12.11\n",
      "checking case BraTS2021_01299_2008.03.26\n",
      "checking case BraTS2021_01394_2008.03.26\n",
      "Verifying label values\n",
      "Expected label values are [0, 1, 2, 3, 4]\n",
      "Labels OK\n",
      "Dataset OK\n",
      "BraTS2021_00122_2008.03.26\n",
      "BraTS2021_01200_2008.03.26\n",
      "BraTS2021_01200_2008.12.11\n",
      "BraTS2021_01299_2008.03.26\n",
      "BraTS2021_01394_2008.03.26\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 138, 168, 135) spacing: before crop: (4, 155, 240, 240) after crop: (4, 142, 162, 135) spacing: [1. 1. 1.] \n",
      "\n",
      "[1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 130, 159, 123) spacing: [1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 130, 159, 123) spacing: [1. 1. 1.] \n",
      "\n",
      "before crop: (4, 155, 240, 240) after crop: (4, 139, 161, 136) spacing: [1. 1. 1.] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Task569_MultPathTest\n",
      "number of threads:  (8, 8) \n",
      "\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "Are we using the nonzero mask for normalization? OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "the median shape of the dataset is  [138. 161. 135.]\n",
      "the max shape in the dataset is  [142. 168. 136.]\n",
      "the min shape in the dataset is  [130. 159. 123.]\n",
      "we don't want feature maps smaller than  4  in the bottleneck\n",
      "the transposed median shape of the dataset is  [138. 161. 135.]\n",
      "generating configuration for 3d_fullres\n",
      "{0: {'batch_size': 2, 'num_pool_per_axis': [5, 5, 5], 'patch_size': array([128, 128, 128]), 'median_patient_size_in_voxels': array([138, 161, 135]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}}\n",
      "transpose forward [0, 1, 2]\n",
      "transpose backward [0, 1, 2]\n",
      "Initializing to run preprocessing\n",
      "npz folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_cropped_data/Task569_MultPathTest\n",
      "output_folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 130, 159, 123)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 130, 159, 123)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 130, 159, 123)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 130, 159, 123)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 139, 161, 136)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 139, 161, 136)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 138, 168, 135)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 138, 168, 135)} \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 142, 162, 135)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 142, 162, 135)} \n",
      "\n",
      "1 10000\n",
      "1 10000\n",
      "2 10000\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_01200_2008.03.26.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_01200_2008.12.11.npz\n",
      "1 10000\n",
      "1 317\n",
      "2 10000\n",
      "1 6056\n",
      "2 10000\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_00122_2008.03.26.npz\n",
      "4 4409\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_01394_2008.03.26.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_stage0/BraTS2021_01299_2008.03.26.npz\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "using nonzero mask for normalization\n",
      "Are we using the nonzero mask for normalization? OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "the median shape of the dataset is  [138. 161. 135.]\n",
      "the max shape in the dataset is  [142. 168. 136.]\n",
      "the min shape in the dataset is  [130. 159. 123.]\n",
      "we don't want feature maps smaller than  4  in the bottleneck\n",
      "the transposed median shape of the dataset is  [138. 161. 135.]\n",
      "[{'batch_size': 24, 'num_pool_per_axis': [5, 5], 'patch_size': array([192, 160]), 'median_patient_size_in_voxels': array([138, 161, 135]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}]\n",
      "Initializing to run preprocessing\n",
      "npz folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_cropped_data/Task569_MultPathTest\n",
      "output_folder: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest\n",
      "no resampling necessaryno resampling necessary\n",
      "\n",
      "no resampling necessaryno resampling necessary\n",
      "\n",
      "before:before:  {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 130, 159, 123)}{'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 130, 159, 123)}  \n",
      "after: \n",
      "after:   {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 130, 159, 123)} \n",
      "\n",
      "{'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 130, 159, 123)} \n",
      "\n",
      "normalization...\n",
      "normalization...\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 139, 161, 136)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 139, 161, 136)} \n",
      "\n",
      "normalization...\n",
      "normalization done\n",
      "normalization done\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 142, 162, 135)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 142, 162, 135)} \n",
      "\n",
      "normalization...\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (4, 138, 168, 135)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (4, 138, 168, 135)} \n",
      "\n",
      "normalization...\n",
      "normalization done\n",
      "1 10000\n",
      "1 10000\n",
      "2 10000\n",
      "2 10000\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_01200_2008.03.26.npz\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_01200_2008.12.11.npz\n",
      "normalization done\n",
      "1 10000\n",
      "2 10000\n",
      "normalization done\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_00122_2008.03.26.npz\n",
      "1 6056\n",
      "2 10000\n",
      "1 317\n",
      "4 10000\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_01299_2008.03.26.npz\n",
      "2 10000\n",
      "4 4409\n",
      "saving:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0/BraTS2021_01394_2008.03.26.npz\n",
      "\n",
      "###########\n",
      "Deleting 2D data directory at: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1_2D_stage0 \n",
      "##############\n",
      "\n",
      "\n",
      "######### COPYING INITIAL MODEL FILES INTO COLLABORATOR 0 FOLDERS #########\n",
      "\n",
      "###########\n",
      "Starting training for task: Task569_MultPathTest\n",
      "\n",
      "###############################################\n",
      "I am running the following nnUNet: 3d_fullres\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  4\n",
      "modalities:  {0: '_0000', 1: '_0001', 2: '_0002', 3: '_0003'}\n",
      "use_mask_for_norm OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'nonCT'), (1, 'nonCT'), (2, 'nonCT'), (3, 'nonCT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [5, 5, 5], 'patch_size': array([128, 128, 128]), 'median_patient_size_in_voxels': array([138, 161, 135]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using sample dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1\n",
      "###############################################\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2024-06-03 11:58:53.031217: Using splits from existing split file: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/splits_final.pkl\n",
      "2024-06-03 11:58:53.033400: The split file contains 5 splits.\n",
      "2024-06-03 11:58:53.034068: Desired fold for training: 0\n",
      "2024-06-03 11:58:53.034458: This split has 4 training and 1 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "2024-06-03 11:58:54.631194: loading checkpoint /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task569_MultPathTest/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Generic_UNet:\n\tsize mismatch for tu.0.weight: copying a param with shape torch.Size([320, 320, 2, 2, 1]) from checkpoint, the shape in current model is torch.Size([320, 320, 2, 2, 2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 81\u001b[0m\n\u001b[1;32m     65\u001b[0m setup_fedsim(postopp_pardirs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/raid/edwardsb/projects/RANO/test_data_links_random_times_1\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     66\u001b[0m          first_three_digit_task_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m569\u001b[39m,\n\u001b[1;32m     67\u001b[0m          init_model_path\u001b[38;5;241m=\u001b[39minit_model_path_568,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m          cuda_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     78\u001b[0m          verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Now try to train and see we get a shape mismatch (note I need to replace the new default value for p below as before we used the default plans identifier but now the code useds a POSTOPP specific string)\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mtrain_on_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTask569_MultPathTest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mtrain_on_task\u001b[0;34m(task, continue_training, current_epoch, without_data_unpacking, use_compressed_data)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_on_task\u001b[39m(task, continue_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, current_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, without_data_unpacking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_compressed_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m###########\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training for task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mtrain_nnunet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnetwork_trainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnetwork_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mcontinue_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinue_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_compressed_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_compressed_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/be-SATGOpenFL/openfl/federated/task/nnunet_v1_with_old_plans_name_hardcoded.py:243\u001b[0m, in \u001b[0;36mtrain_nnunet\u001b[0;34m(epochs, current_epoch, network, network_trainer, task, fold, continue_training, validation_only, c, p, use_compressed_data, deterministic, npz, find_lr, valbest, fp32, val_folder, disable_saving, disable_postprocessing_on_folds, val_disable_overwrite, disable_next_stage_pred, pretrained_weights)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validation_only:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;66;03m# -c was set, continue a previous training and ignore pretrained weights\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m         \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_latest_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcontinue_training) \u001b[38;5;129;01mand\u001b[39;00m (args\u001b[38;5;241m.\u001b[39mpretrained_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# we start a new training. If pretrained_weights are set, use them\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         load_pretrained_weights(trainer\u001b[38;5;241m.\u001b[39mnetwork, args\u001b[38;5;241m.\u001b[39mpretrained_weights)\n",
      "File \u001b[0;32m~/repositories/nnUNet/nnunet/training/network_training/network_trainer.py:302\u001b[0m, in \u001b[0;36mNetworkTrainer.load_latest_checkpoint\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_latest_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfile(join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_final_checkpoint.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_final_checkpoint.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfile(join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_latest.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_checkpoint(join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_latest.model\u001b[39m\u001b[38;5;124m\"\u001b[39m), train\u001b[38;5;241m=\u001b[39mtrain)\n",
      "File \u001b[0;32m~/repositories/nnUNet/nnunet/training/network_training/network_trainer.py:321\u001b[0m, in \u001b[0;36mNetworkTrainer.load_checkpoint\u001b[0;34m(self, fname, train)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# saved_model = torch.load(fname, map_location=torch.device('cuda', torch.cuda.current_device()))\u001b[39;00m\n\u001b[1;32m    320\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(fname, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 321\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint_ram\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/nnUNet/nnunet/training/network_training/network_trainer.py:365\u001b[0m, in \u001b[0;36mNetworkTrainer.load_checkpoint_ram\u001b[0;34m(self, checkpoint, train)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamp_grad_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp_grad_scaler\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamp_grad_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "File \u001b[0;32m~/virtual/be-SATGOpenFL-too/lib/python3.8/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generic_UNet:\n\tsize mismatch for tu.0.weight: copying a param with shape torch.Size([320, 320, 2, 2, 1]) from checkpoint, the shape in current model is torch.Size([320, 320, 2, 2, 2])."
     ]
    }
   ],
   "source": [
    "# NOTE: This cell takes about 8 minutes to run on spr01 (Titan XP)\n",
    "\n",
    "# First do what we did before\n",
    "\n",
    "# Some paths below will be under my folders, but someone else might want to replace this part\n",
    "test_pardir = '/raid/edwardsb/projects/RANO'\n",
    "\n",
    "\n",
    "datadir_568 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_preprocessed', 'Task568_MultPathTest')\n",
    "# next two for deleting only\n",
    "alt_1_datadir_568 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_raw_data', 'Task568_MultPathTest')\n",
    "alt_2_datadir_568 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_cropped_data', 'Task568_MultPathTest')\n",
    "\n",
    "model_pre_dir_568 = os.path.join(test_pardir, 'NNUnetModels', 'nnUNet', '3d_fullres', 'Task568_MultPathTest')\n",
    "modeldir_568 = os.path.join(model_pre_dir_568, 'nnUNetTrainerV2__nnUNetPlansv2.1', 'fold_0')\n",
    "\n",
    "datadir_569 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_preprocessed', 'Task569_MultPathTest')\n",
    "# next two for deleting only\n",
    "alt_1_datadir_569 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_raw_data', 'Task569_MultPathTest')\n",
    "alt_2_datadir_569 = os.path.join(test_pardir, 'BraTS22_pretending_tobe_postopp', 'nnUNet_raw_data_base', 'nnUNet_cropped_data', 'Task569_MultPathTest')\n",
    "\n",
    "model_pre_dir_569 = os.path.join(test_pardir, 'NNUnetModels', 'nnUNet', '3d_fullres', 'Task569_MultPathTest')\n",
    "model_dir_569 = os.path.join(model_pre_dir_569, 'nnUNetTrainerV2__nnUNetPlansv2.1', 'fold_0')\n",
    "\n",
    "# At this point deleted some folders via linux command line\n",
    "\n",
    "rm_tree(datadir_568)\n",
    "rm_tree(alt_1_datadir_568)\n",
    "rm_tree(alt_2_datadir_568)\n",
    "rm_tree(model_pre_dir_568)\n",
    "\n",
    "\n",
    "rm_tree(datadir_569)\n",
    "rm_tree(alt_1_datadir_569)\n",
    "rm_tree(alt_2_datadir_569)\n",
    "rm_tree(model_pre_dir_569)\n",
    "\n",
    "\n",
    "setup_fedsim(postopp_pardirs=['/raid/edwardsb/projects/RANO/test_data_links_random_times_0'], \n",
    "         first_three_digit_task_num=568,\n",
    "         init_model_path=None,\n",
    "         init_model_info_path= None, \n",
    "         task_name='MultPathTest', \n",
    "         percent_train=0.8, \n",
    "         split_logic='by_subject_time_pair', \n",
    "         network='3d_fullres', \n",
    "         network_trainer='nnUNetTrainerV2', \n",
    "         fold=0,  \n",
    "         timestamp_selection='all', \n",
    "         num_institutions=1, \n",
    "         cuda_device='5', \n",
    "         verbose=False)\n",
    "\n",
    "# The source for the init model paths are located in the 568 results folder\n",
    "init_model_path_568 = os.path.join(modeldir_568, 'model_initial_checkpoint.model') \n",
    "init_model_info_path_568 =  os.path.join(modeldir_568, 'model_initial_checkpoint.model.pkl') \n",
    "\n",
    "# The plan paths are under the data folders\n",
    "plan_path_568 = os.path.join(datadir_568, 'nnUNetPlansv2.1_plans_3D.pkl')\n",
    "plan_path_569 = os.path.join(datadir_569, 'nnUNetPlansv2.1_plans_3D.pkl')\n",
    "\n",
    "# Now set up 569 as we did in the first fed test, with the old script that passes only initial model and initial model info bu only shares plan across the first collaborator\n",
    "# in the call with the others (though there are no others here)\n",
    "\n",
    "setup_fedsim(postopp_pardirs=['/raid/edwardsb/projects/RANO/test_data_links_random_times_1'], \n",
    "         first_three_digit_task_num=569,\n",
    "         init_model_path=init_model_path_568,\n",
    "         init_model_info_path= init_model_info_path_568,\n",
    "         task_name='MultPathTest', \n",
    "         percent_train=0.8, \n",
    "         split_logic='by_subject_time_pair', \n",
    "         network='3d_fullres', \n",
    "         network_trainer='nnUNetTrainerV2', \n",
    "         fold=0,  \n",
    "         timestamp_selection='all', \n",
    "         num_institutions=1, \n",
    "         cuda_device='5', \n",
    "         verbose=False)\n",
    "\n",
    "# Now try to train and see we get a shape mismatch (note I need to replace the new default value for p below as before we used the default plans identifier but now the code useds a POSTOPP specific string)\n",
    "train_on_task(task='Task569_MultPathTest', current_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "Starting training for task: Task569_MultPathTest\n",
      "\n",
      "###############################################\n",
      "I am running the following nnUNet: 3d_fullres\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  4\n",
      "modalities:  {0: '_0000', 1: '_0001', 2: '_0002', 3: '_0003'}\n",
      "use_mask_for_norm OrderedDict([(0, True), (1, True), (2, True), (3, True)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'nonCT'), (1, 'nonCT'), (2, 'nonCT'), (3, 'nonCT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [5, 5, 4], 'patch_size': array([128, 160, 112]), 'median_patient_size_in_voxels': array([137, 171, 130]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using sample dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/nnUNetData_plans_v2.1\n",
      "###############################################\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2024-06-03 12:03:43.031696: Using splits from existing split file: /raid/edwardsb/projects/RANO/BraTS22_pretending_tobe_postopp/nnUNet_raw_data_base/nnUNet_preprocessed/Task569_MultPathTest/splits_final.pkl\n",
      "2024-06-03 12:03:43.034406: The split file contains 5 splits.\n",
      "2024-06-03 12:03:43.035027: Desired fold for training: 0\n",
      "2024-06-03 12:03:43.035633: This split has 4 training and 1 validation cases.\n",
      "unpacking dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "2024-06-03 12:03:43.852579: loading checkpoint /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task569_MultPathTest/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True\n",
      "2024-06-03 12:03:44.302680: lr: 0.009991\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-06-03 12:03:56.958957: Unable to plot network architecture:\n",
      "2024-06-03 12:03:56.968052: No module named 'hiddenlayer'\n",
      "2024-06-03 12:03:56.968819: \n",
      "printing the network instead:\n",
      "\n",
      "2024-06-03 12:03:56.969363: Generic_UNet(\n",
      "  (conv_blocks_localization): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_blocks_context): ModuleList(\n",
      "    (0): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(4, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (td): ModuleList()\n",
      "  (tu): ModuleList(\n",
      "    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 1), stride=(2, 2, 1), bias=False)\n",
      "    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "    (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
      "  )\n",
      "  (seg_outputs): ModuleList(\n",
      "    (0): Conv3d(320, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): Conv3d(256, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (2): Conv3d(128, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (3): Conv3d(64, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (4): Conv3d(32, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "2024-06-03 12:03:56.975149: \n",
      "\n",
      "2024-06-03 12:03:56.978701: \n",
      "epoch:  1\n",
      "2024-06-03 12:09:20.898035: train loss : -0.5564\n",
      "2024-06-03 12:09:41.091434: validation loss: -0.1116\n",
      "2024-06-03 12:09:41.094513: Average global foreground Dice: [0.3339, 0.6898, 0.8043]\n",
      "2024-06-03 12:09:41.095442: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edwardsb/repositories/nnUNet/nnunet/training/network_training/nnUNetTrainer.py:719: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  global_dc_per_class = [i for i in [2 * i / (2 * i + j + k) for i, j, k in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 12:09:41.556391: lr: 0.009982\n",
      "2024-06-03 12:09:41.557259: This epoch took 344.577959 s\n",
      "\n",
      "2024-06-03 12:09:41.641403: saving checkpoint to /raid/edwardsb/projects/RANO/NNUnetModels/nnUNet/3d_fullres/Task569_MultPathTest/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model...\n",
      "2024-06-03 12:09:42.202568: done, saving took 0.64 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now drop in the plans file that should have been shared and see that training works now\n",
    "\n",
    "shutil.copyfile(src=plan_path_568,dst=plan_path_569)\n",
    "\n",
    "# Now try to train again and this time see it works\n",
    "train_on_task(task='Task569_MultPathTest', current_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see sucessful data processing and model definitions with the new scripts, run the sript: test_fl_setup.py in the same folder holding this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "be-SATGOpenFL-too",
   "language": "python",
   "name": "be-satgopenfl-too"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
